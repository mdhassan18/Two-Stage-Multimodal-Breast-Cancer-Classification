# -*- coding: utf-8 -*-
"""Demo_GradCAM_Gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VC5wxZb1kPXwkQ00HbEy0cBZS9DqCbf9
"""

# CELL #1
from google.colab import drive
drive.mount('/content/drive')

# CELL #2
import os
import torch
import torch.nn as nn
import numpy as np
from torchvision import transforms
from PIL import Image
import timm
import gradio as gr

# CELL #3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# CELL #4

stage1_model = timm.create_model(
    "efficientnet_b0",
    pretrained=False,
    num_classes=2
)

stage1_model.load_state_dict(
    torch.load(
        "/content/drive/MyDrive/BreastCancer_AI_Project/models/stage1/stage1_screening.pth",
        map_location=device
    )
)

stage1_model = stage1_model.to(device)
stage1_model.eval()

print("Stage-1 model loaded")

# CELL #5

class TextEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 32),
            nn.ReLU(),
            nn.Linear(32, 64)
        )

    def forward(self, x):
        return self.net(x)


mammogram_encoder = timm.create_model(
    "efficientnet_b0", pretrained=False, num_classes=0
).to(device)

ultrasound_encoder = timm.create_model(
    "mobilenetv3_small_100", pretrained=False, num_classes=0
).to(device)


class LateFusionModel(nn.Module):
    def __init__(self, mam_dim, us_dim, txt_dim):
        super().__init__()
        self.text_encoder = TextEncoder()
        fused_dim = mam_dim + us_dim + txt_dim
        self.attention = nn.Linear(fused_dim, 3)
        self.classifier = nn.Linear(fused_dim, 2)

    def forward(self, mam, us, text):
        f_mam = mammogram_encoder(mam)
        f_us = ultrasound_encoder(us)
        f_txt = self.text_encoder(text)

        fused = torch.cat([f_mam, f_us, f_txt], dim=1)
        attn = torch.softmax(self.attention(fused), dim=1)
        out = self.classifier(fused)
        return out, attn

# CELL #6

with torch.no_grad():
    dummy = torch.randn(1,3,224,224).to(device)
    dummy_txt = torch.randn(1,2).to(device)

    mam_dim = mammogram_encoder(dummy).shape[1]
    us_dim = ultrasound_encoder(dummy).shape[1]
    txt_dim = TextEncoder().to(device)(dummy_txt).shape[1]

stage2_model = LateFusionModel(mam_dim, us_dim, txt_dim).to(device)

stage2_model.load_state_dict(
    torch.load(
        "/content/drive/MyDrive/BreastCancer_AI_Project/models/stage2/stage2_multimodal_final.pth",
        map_location=device
    )
)

stage2_model.eval()

print("Stage-2 model loaded")

# CELL #7
img_tfms = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor()
])

# CELL #8 — Final Inference with High-Quality Grad-CAM++ & Explainability

def overlay_cam(image_np, cam):
    heatmap = cv2.applyColorMap(
        np.uint8(255 * cam),
        cv2.COLORMAP_TURBO  # better than JET for medical images
    )
    heatmap = heatmap[..., ::-1] / 255.0  # BGR → RGB

    overlay = 0.6 * image_np + 0.4 * heatmap
    overlay = np.clip(overlay, 0, 1)
    return (overlay * 255).astype(np.uint8)


def generate_explainability_text(cam, modality="mammogram"):
    h, w = cam.shape
    y, x = np.unravel_index(np.argmax(cam), cam.shape)

    vertical = "upper" if y < h/3 else "middle" if y < 2*h/3 else "lower"
    horizontal = "left" if x < w/3 else "central" if x < 2*w/3 else "right"

    if modality == "mammogram":
        return (
            f"The model focused on a high-activation region in the "
            f"{vertical} {horizontal} area of the mammogram, "
            f"indicating a suspected region of abnormality."
        )
    else:
        return (
            f"The ultrasound analysis shows prominent activation in the "
            f"{vertical} {horizontal} region, suggesting a suspicious lesion."
        )


def run_inference(mammo_img, us_img, age, birads):
    try:
        if mammo_img is None or us_img is None:
            return "Missing image", "—", {}, None, None, ""

        # ---------- Mammogram ----------
        mam_img = Image.fromarray(mammo_img.astype("uint8")).convert("RGB")
        mam_tensor = img_tfms(mam_img).unsqueeze(0).to(device)

        with torch.no_grad():
            s1_logits = stage1_model(mam_tensor)
            s1_probs = torch.softmax(s1_logits, dim=1)[0]

        stage1_pred = "Abnormal" if s1_probs[1] > 0.5 else "Normal"
        stage1_conf = float(torch.max(s1_probs))

        # ---------- Ultrasound ----------
        us_img_pil = Image.fromarray(us_img.astype("uint8")).convert("RGB")
        us_tensor = img_tfms(us_img_pil).unsqueeze(0).to(device)

        # ---------- Text ----------
        text_tensor = torch.tensor([[age, birads]], dtype=torch.float32).to(device)

        # ---------- Stage-2 ----------
        with torch.no_grad():
            s2_logits, attn = stage2_model(mam_tensor, us_tensor, text_tensor)
            s2_probs = torch.softmax(s2_logits, dim=1)[0]

        stage2_pred = "Malignant" if s2_probs[1] > 0.5 else "Benign"
        stage2_conf = float(torch.max(s2_probs))

        # ---------- Grad-CAM++ ----------
        mam_cam = generate_gradcam(mammogram_encoder, mam_target_layer, mam_tensor)
        us_cam = generate_gradcam(ultrasound_encoder, us_target_layer, us_tensor)

        mam_np = np.array(mam_img.resize((224, 224))) / 255.0
        us_np = np.array(us_img_pil.resize((224, 224))) / 255.0

        mam_heatmap = overlay_cam(mam_np, mam_cam)
        us_heatmap = overlay_cam(us_np, us_cam)

        explanation = (
            generate_explainability_text(mam_cam, "mammogram")
            + "\n"
            + generate_explainability_text(us_cam, "ultrasound")
        )

        attn_vals = {
            "Mammogram": float(attn[0][0]),
            "Ultrasound": float(attn[0][1]),
            "Text": float(attn[0][2])
        }

        return (
            f"{stage1_pred} (Confidence: {stage1_conf:.2f})",
            f"{stage2_pred} (Confidence: {stage2_conf:.2f})",
            attn_vals,
            mam_heatmap,
            us_heatmap,
            explanation
        )

    except Exception as e:
        return "ERROR", str(e), {}, None, None, ""

# CELL #10 — Install Grad-CAM++
!pip install -q grad-cam

# CELL #11 — Grad-CAM imports

from pytorch_grad_cam import GradCAMPlusPlus
from pytorch_grad_cam.utils.image import show_cam_on_image
import cv2

# CELL #12 — Improved Target Layers for Clearer Grad-CAM++

# EfficientNet-B0 (Mammogram)
mam_target_layer = mammogram_encoder.blocks[-3]

# MobileNetV3 (Ultrasound)
us_target_layer = ultrasound_encoder.blocks[-2]

print("Using higher-resolution target layers for Grad-CAM++")

# CELL #13 — Improved Grad-CAM++ Generator (Normalized + Smoothed)

from pytorch_grad_cam import GradCAMPlusPlus
import cv2
import numpy as np

def generate_gradcam(model, target_layer, input_tensor):
    cam = GradCAMPlusPlus(
        model=model,
        target_layers=[target_layer]
    )

    cam_map = cam(input_tensor=input_tensor)[0]

    # Normalize CAM
    cam_map = np.maximum(cam_map, 0)
    cam_map = cam_map / (cam_map.max() + 1e-8)

    # Light smoothing for visual clarity
    cam_map = cv2.GaussianBlur(cam_map, (3, 3), 0)

    return cam_map

# CELL #14 — Explainability text logic

def generate_explainability_text(cam, modality="mammogram"):
    h, w = cam.shape
    y, x = np.unravel_index(np.argmax(cam), cam.shape)

    vertical = "upper" if y < h/3 else "middle" if y < 2*h/3 else "lower"
    horizontal = "left" if x < w/3 else "central" if x < 2*w/3 else "right"

    if modality == "mammogram":
        return (
            f"The model focused on a high-activation region in the "
            f"{vertical} {horizontal} area of the mammogram, "
            f"indicating a suspected region of abnormality."
        )
    else:
        return (
            f"The ultrasound analysis shows prominent activation in the "
            f"{vertical} {horizontal} region, suggesting a suspicious lesion."
        )

# CELL #15 — FINAL GRADIO INTERFACE WITH GRAD-CAM++

gr.Interface(
    fn=run_inference,
    inputs=[
        gr.Image(label="Mammogram Image", type="numpy"),
        gr.Image(label="Ultrasound Image", type="numpy"),
        gr.Number(label="Age", value=45),
        gr.Number(label="BIRADS", value=3)
    ],
    outputs=[
        gr.Textbox(label="Stage-1 Screening Result"),
        gr.Textbox(label="Stage-2 Diagnosis Result"),
        gr.JSON(label="Attention Weights"),
        gr.Image(label="Mammogram Grad-CAM++"),
        gr.Image(label="Ultrasound Grad-CAM++"),
        gr.Textbox(label="Explainability Summary")
    ],
    title="Two-Stage Explainable AI System for Breast Cancer Detection",
    description="""
    Grad-CAM++ highlights spatial regions influencing predictions.
    Attention weights show modality importance.
    Designed for clinical interpretability.
    """,
    allow_flagging="never"
).launch()